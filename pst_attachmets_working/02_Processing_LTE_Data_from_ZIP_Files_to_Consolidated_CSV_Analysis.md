# [Umer Saeed](https://www.linkedin.com/in/engumersaeed/)  
**Senior RF Planning & Optimization Engineer**  


ğŸ“ **Location:** Dream Gardens, Defence Road, Lahore  
ğŸ“ **Mobile:** +92 301 8412180  
âœ‰ **Email:** [umersaeed81@hotmail.com](mailto:umersaeed81@hotmail.com)  

## **Education**  
ğŸ“ **BSc Telecommunications Engineering** â€“ School of Engineering  
ğŸ“ **MS Data Science** â€“ School of Business and Economics  
**University of Management & Technology**  

-----------------------------------------------------------------------------------

# ğŸ“‚ Processing LTE Data from ZIP Files to Consolidated CSV Analysis ğŸ“Š

This script processes **multiple ZIP files**, each containing **multiple CSV files**. It scans all ZIP files in the specified folder and extracts only those CSV files that have "**LTE_DA**" in their filename. The goal is to **read specific CSV files**, clean and format the extracted data, and merge it with a site list to ensure only relevant records are included. The data is then summarized, analyzed, and used to generate a **final report** for performance evaluation. The processed results are saved as an **Excel file** for further analysis.

![](https://github.com/Umersaeed81/Python_For_RF_Optimization_And_Planning_Engineer/blob/main/pst_attachmets_working/PIC_06n.png?raw=true)

### ğŸ—‚ï¸ Import Required Libraries


```python
import os                         # ğŸ“ Handle file system operations  
import zipfile                    # ğŸ“¦ Work with ZIP files  
import numpy as np                # ğŸ”¢ Numerical computations  
import pandas as pd               # ğŸ“Š Data manipulation  
from pathlib import Path          # ğŸ›¤ï¸ Handle file paths  
```

- `os` â†’ Helps with file and directory handling ğŸ“‚
- `zipfile` â†’ Extracts data from ZIP archives ğŸ“¦
- `numpy` (np) â†’ Used for numerical operations ğŸ”¢
- `pandas` (pd) â†’ Processes CSV & Excel data ğŸ“Š
- `pathlib.Path` â†’ Helps find ZIP files in a folder ğŸ“

### ğŸ“‚ Define ZIP Folder Path


```python
zip_folder = "E:/PRS_Email/Attachments"  # ğŸ“Œ Set the directory containing ZIP files  
```

- This is the folder where all ZIP files are stored ğŸ“‚

### ğŸ” Retrieve ZIP Files


```python
# ğŸ“„ Get a list of all ZIP files in the specified folder  
zip_files = list(Path(zip_folder).glob("*.zip"))  
```

- This finds all `.zip` files inside the folder ğŸ“¦

### ğŸ“¥ Extract and Read CSV Files from ZIP Archives


```python
dfs = []  # ğŸ“ Initialize an empty list to store DataFrames  

# ğŸ¯ Define the required columns  
required_columns = ['Date','Cell Name','eNodeB Function Name','Frequency band',
                    'Downlink EARFCN','Data Volume (GB)','DL User Thrp (Mbps)_DEN',
                    'DL User Thrp (Mbps)_NUM']

# ğŸ”„ Loop through each ZIP file  
for zip_file in zip_files:
    with zipfile.ZipFile(zip_file, 'r') as z:
        # ğŸ“ƒ List CSV files containing "LTE_DA"  
        csv_files = [f for f in z.namelist() if "LTE_DA" in f and f.endswith(".csv")]
        
        for csv_file in csv_files:
            with z.open(csv_file) as f:
                try:
                    # ğŸ“Š Read required columns, parse 'Date' as datetime  
                    df = pd.read_csv(f, usecols=required_columns, parse_dates=["Date"],skiprows=range(6),\
                                skipfooter=1,engine='python',na_values=['NIL','/0'])
                    df["source_file"] = csv_file  # ğŸ·ï¸ Add file name tracking  
                    df["zip_file"] = zip_file.name  # ğŸ·ï¸ Add ZIP file tracking  
                    dfs.append(df)  # ğŸ“Œ Store DataFrame  
                except ValueError:  
                    # ğŸš¨ Handle missing columns  
                    print(f"Skipping {csv_file} in {zip_file.name} - Missing required columns.")

# ğŸ“Œ Combine all extracted data into one DataFrame  
if dfs:
    final_df = pd.concat(dfs, ignore_index=True)  # ğŸ”„ Merge DataFrames  
else:
    print("No matching CSV files found or required columns missing.")  # âŒ Show error message  
```

- ğŸ“„ **Reads CSV files from each ZIP** to extract relevant data.
- ğŸ§ **Filters files containing "LTE_DA"** to focus on relevant datasets.
- ğŸ“Š **Extracts only the required columns** for efficient processing.
- âŒ Handles missing values (`NIL`, `/0`) by converting them to `NaN`.
- ğŸ“‹ **Merges all extracted CSV data into a single table** (f`inal_df`).
- âœ… Skips footer rows using `skipfooter=1` to exclude unwanted summary rows.
- ğŸ·ï¸ **Creates a 'source_file' column** to keep track of the original CSV file name.
- ğŸ·ï¸ **Creates a 'zip_file' column** to record the source ZIP file for each dataset.
- ğŸš¨ **Notifies the user** when a file is skipped due to missing required columns, explicitly mentioning the affected file name.
- ğŸ“… **Parses 'Date' column** as a datetime format to ensure consistency in date-related operations.
- ğŸ›  Uses `engine=python` to handle complex file structures, such as skipping rows and footers.
- ğŸ”„ **Concatenates all DataFrames** into a single DataFrame (`final_df`) for easy analysis.
- âŒ **Displays an error message** when no valid CSV files are found or all files lack required columns.

### âœï¸ Clean Column Values


```python
# ğŸ§¹ Remove extra spaces in 'eNodeB Function Name'  
final_df['eNodeB Function Name'] = final_df['eNodeB Function Name'].str.replace(r'\s+', ' ', regex=True)

# ğŸ§¹ Remove extra spaces in 'Cell Name'  
final_df['Cell Name'] = final_df['Cell Name'].str.replace(r'\s+', ' ', regex=True)
```

- Replaces multiple spaces with a single space in text fields ğŸ§¹

### ğŸ·ï¸ Generate Site and Cell IDs


```python
# ğŸ—ï¸ Extract LTE Site ID from 'eNodeB Function Name'  
final_df['LTE_Site_ID'] = final_df['eNodeB Function Name'].str.split('_').str[0]

# ğŸ—ï¸ Extract LTE Cell ID from 'Cell Name'  
final_df['LTE_Cell_ID'] = final_df['Cell Name'].str.split('_').str[0]
```

- Generates unique Site ID and Cell ID from names ğŸ“Œ

### ğŸ“‚ Set Working Directory


```python
path = 'E:/PRS_Email/req_sites'  # ğŸ“Œ Define path  
os.chdir(path)  # ğŸ”„ Change current working directory  
```

- Moves to the folder where we will process the data ğŸ“‚

### ğŸ“‘ Load Required Site List


```python
# ğŸ“¥ Read the site list Excel file  
req_site_list = pd.read_excel('Site_List.xlsx')
```

- Loads **site reference data** from an Excel file ğŸ“–

### ğŸ”„ Merge Site List with Extracted Data


```python
# ğŸ”— Merge required site list with extracted dataset  
data_set = pd.merge(req_site_list,final_df,how='left',on=['LTE_Site_ID'])
```

- Keeps only the required sites ğŸ“
- Joins data using LTE_Site_ID as the key ğŸ”‘

### ğŸ“Š Aggregate KPI Metrics per Day


```python
# ğŸ“… Group by 'Date', 'LTE_Site_ID', 'Province', 'Site integration Date' and calculate sum of KPIs  
col_sum_kpis_day= data_set.groupby(['Date','LTE_Site_ID','Province','Site integration Date'])\
        [['Data Volume (GB)',\
          'DL User Thrp (Mbps)_DEN','DL User Thrp (Mbps)_NUM']]\
        .sum().reset_index()
```

- Group data by day & site ğŸ“†
- Sums important KPIs ğŸ“Š

### ğŸ“Calculate Downlink User Throughput


```python
# ğŸ“ˆ Compute 'DL User Thrp (Mbps)'  
col_sum_kpis_day['DL User Thrp (Mbps)'] =  (col_sum_kpis_day['DL User Thrp (Mbps)_NUM'] / col_sum_kpis_day['DL User Thrp (Mbps)_DEN']) / 1024
```

- Computes throughput in Mbps ğŸ”¢

### ğŸ“† Generate Week Identifier (WK)


```python
# ğŸ—“ï¸ Create 'WK' column for Year_Week format  
col_sum_kpis_day['WK'] = col_sum_kpis_day['Date'].dt.isocalendar().year.astype(str) + "_" + \
           col_sum_kpis_day['Date'].dt.isocalendar().week.astype(str).apply(lambda x: x.zfill(2))
```

- Creates Week labels for analysis ğŸ“…

### ğŸ“… Generate Month Identifier


```python
# ğŸ“† Create 'Month' column for Year_Month format  
col_sum_kpis_day['Month'] = col_sum_kpis_day['Date'].dt.year.astype(str) + "_" + \
               col_sum_kpis_day['Date'].dt.month.astype(str).apply(lambda x: x.zfill(2))
```

- Creates Month labels for analysis ğŸ“…

### ğŸš¦Identify Sites Below KPI Threshold


```python
# ğŸš¨ Check if 'DL User Thrp (Mbps)' is below 4 and mark as 'Yes' or 'No'  
col_sum_kpis_day['DL User Thrp (Mbps)<4(Yes/No)'] = np.where(
                    (col_sum_kpis_day['DL User Thrp (Mbps)'] < 4),
                    'Yes', 'No')
```

- Flags sites with low throughput (<4 Mbps) ğŸš¨

### ğŸ¯ Select Final Columns for Output


```python
# ğŸ“Œ Keep only necessary columns for final dataset  
col_sum_kpis_day_final = col_sum_kpis_day[['Date','WK','Month','LTE_Site_ID', 'Province',
                        'Site integration Date','Data Volume (GB)', 'DL User Thrp (Mbps)_DEN',
       'DL User Thrp (Mbps)_NUM', 'DL User Thrp (Mbps)','DL User Thrp (Mbps)<4(Yes/No)']]
```

- Selects only **important columns** for reporting ğŸ“Š

### ğŸ’¾ Export Final Data to Excel


```python
col_sum_kpis_day_final.to_excel('DL_User_Thrp.xlsx',index=False)
```

- Exports final report to Excel ğŸ“¤
